---
title: "Problem Set 11"
author: "Noah Seaner"
date: "4/29/2025"
output: pdf_document
---

```{r}
library(nimble)
library(coda)
library(readr)
library(dplyr)
library(MCMCvis)
```

# Problem 1

## a.

```{r eval=FALSE}
awards=read.csv('school_awards.csv')
n=length(awards$X)

awards <- awards %>%
  mutate(Private=ifelse(School=='private',0,1),
         Public=ifelse(School=='public',0,1))

y <- awards$Num_Awards
pub <- awards$Public
priv <- awards$Private
avgmath <- awards$Avg_Math

code <- nimbleCode({
  b0 ~ dnorm(0,0.001)
  bmath ~ dnorm(0,0.001)
  bpublic ~ dnorm(0,0.001)
  bprivate ~ dnorm(0,0.001)
  
  for(i in 1:n){
    log(lambda[i]) <- b0+bmath*avgmath[i]+bprivate*priv[i]+bpublic*pub[i]
    y[i] ~ dpois(lambda[i])
  }
})

constants <- list(n=n)
data <- list(y=y,avgmath=avgmath,priv=priv,pub=pub)
inits <- list(b0=0,bmath=0,bprivate=0,bpublic=0)

Rmodel <- nimbleModel(code,constants,data,inits)

conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc)

samples <- runMCMC(Cmcmc,niter=20000,nburnin=10000)
```

```{r echo=FALSE,eval=FALSE}
save(samples,file='samples_problem12.1.Rdata')
```

```{r echo=FALSE}
load('samples_problem12.1.Rdata')
```

```{r}
effectiveSize(samples)
```
The parameters with the lowest ESS are the intercept and the coefficient for the number of math awards.

## b.

```{r}
acfplot(as.mcmc(samples))
```
As seen in the plot, the intercept and the coefficient for the number of math awards have the slowest auto-correlation.

## c.

```{r eval=FALSE}
awards=read.csv('school_awards.csv')
n=length(awards$X)

awards <- awards %>%
  mutate(Private=ifelse(School=='private',0,1),
         Public=ifelse(School=='public',0,1))

y <- awards$Num_Awards
pub <- awards$Public
priv <- awards$Private
avgmath <- awards$Avg_Math

code <- nimbleCode({
  b0 ~ dnorm(0,0.001)
  bmath ~ dnorm(0,0.001)
  bpublic ~ dnorm(0,0.001)
  bprivate ~ dnorm(0,0.001)
  
  for(i in 1:n){
    log(lambda[i]) <- b0+bmath*avgmath[i]+bprivate*priv[i]+bpublic*pub[i]
    y[i] ~ dpois(lambda[i])
  }
})

constants <- list(n=n)
data <- list(y=y,avgmath=avgmath,priv=priv,pub=pub)
inits <- list(b0=0,bmath=0,bprivate=0,bpublic=0)

Rmodel <- nimbleModel(code,constants,data,inits)

conf <- configureMCMC(Rmodel,onlySlice=TRUE)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc)

samples <- runMCMC(Cmcmc,niter=20000,nburnin=10000)
```

```{r echo=FALSE,eval=FALSE}
save(samples,file='samples2_problem12.1.Rdata')
```

```{r echo=FALSE}
load('samples2_problem12.1.Rdata')
```

```{r}
effectiveSize(samples)
```
The ESS for each parameter under slice sampling increased to more than double the ESS they had under the Metropolis-Hastings random walk samplers.

## d.

```{r}
acfplot(as.mcmc(samples))
```

## e.

```{r}
MCMCtrace(samples,params=c('b0','bmath'),type='trace',pdf=FALSE,iter=1000)
```
It would appear that the two coefficients mix in opposite directions. In other words, when the intercept accepts higher values, bmath accepts lower values.

## f.

```{r}
cor(samples)
```
According to the matrix, b0 and bmath are negatively correlated. Therefore, my guess was correct.

## g.

```{r eval=FALSE}
awards=read.csv('school_awards.csv')
n=length(awards$X)

awards <- awards %>%
  mutate(Private=ifelse(School=='private',0,1),
         Public=ifelse(School=='public',0,1))

y <- awards$Num_Awards
pub <- awards$Public
priv <- awards$Private
avgmath <- awards$Avg_Math-mean(awards$Avg_Math)

code <- nimbleCode({
  b0 ~ dnorm(0,0.001)
  bmath ~ dnorm(0,0.001)
  bpublic ~ dnorm(0,0.001)
  bprivate ~ dnorm(0,0.001)
  
  for(i in 1:n){
    log(lambda[i]) <- b0+bmath*avgmath[i]+bprivate*priv[i]+bpublic*pub[i]
    y[i] ~ dpois(lambda[i])
  }
})

constants <- list(n=n)
data <- list(y=y,avgmath=avgmath,priv=priv,pub=pub)
inits <- list(b0=0,bmath=0,bprivate=0,bpublic=0)

Rmodel <- nimbleModel(code,constants,data,inits)

conf <- configureMCMC(Rmodel,onlySlice=TRUE)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc)

samples <- runMCMC(Cmcmc,niter=20000,nburnin=10000)
```

```{r echo=FALSE,eval=FALSE}
save(samples,file='samples3_problem12.1.Rdata')
```

```{r echo=FALSE}
load('samples3_problem12.1.Rdata')
```

```{r}
effectiveSize(samples)
cor(samples)
```
The ESS for all four parameters are over $100$, and the correlation matrix does not have any values over $0.8$, so centering the math covariate fixed our problem.

## h.

```{r echo=FALSE}
load('samples2_problem12.1.Rdata')
```

```{r}
MCMCtrace(samples,type='density',pdf=FALSE)
```

```{r echo=FALSE}
load('samples3_problem12.1.Rdata')
```

```{r}
MCMCtrace(samples,type='density',pdf=FALSE)
```
Comparing the two MCMC chains we ran in parts f and g, we can see that centering the math covariate clearly affected the posterior distributions. The intercept now has a slight hump at the peak of its curve, the math covariate has a less dramatic left skew, and the public covariate no longer has a right skew and instead looks more normal so to speak. The private covariate is the only variable whose posterior distribution did not have any significant changes.

# Problem 2

## a.

Our priors are $\mu_0\sim Normal(1500,\sigma=500)$, $\sigma_{sample}\sim Uniform(0,10000)$, and $\sigma_{batch}\sim Uniform(0,10000)$. The likelihood is $y\mid\mu_0,\sigma_{sample},\sigma_{batch}\sim Normal(\mu,\sigma_{sample})$.

## b.

The three top-level parameters are $\mu_0$, $\sigma_{sample}$, and $\sigma_{batch}$. $\mu_0$ represents the overall mean of the 30 samples we take, $\sigma_{sample}$ is the variation of samples from the same batch, and $\sigma_{batch}$ is the variation between batches.

## c.

The six latent variables that we have are the means $\mu_i$ for $i=1,2,\dots,5,6$ for each batch of samples. 

## d.

```{r eval=FALSE}
dyes <- read.csv('dyes.csv')

batch <- dyes$batch
N <- length(batch)
nbatch <- max(batch)
y <- dyes$concentration

code <- nimbleCode({
  mu0 ~ dnorm(1500,sd=500)
  sbatch ~ dunif(0,100)
  ssample ~ dunif(0,100)
  for(i in 1:nbatch){
    eps[i] ~ dnorm(0,sd=sbatch)
    mu[i] <- mu0+eps[i]
  }
  for(i in 1:N){
    y[i] ~ dnorm(mu[batch[i]],sd=ssample)
  }
})

constants <- list(nbatch=nbatch,N=N,batch=batch)
data <- list(y=y)
inits <- list(mu0=1500,sbatch=1,ssample=2,eps=rep(0,6))

Rmodel <- nimbleModel(code,constants,data,inits)
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc)

samples <- runMCMC(Cmcmc,niter=20000)
```

```{r echo=FALSE,eval=FALSE}
save(samples,file='samples_problem12.2.Rdata')
```

```{r echo=FALSE}
load('samples_problem12.2.Rdata')
```

```{r}
MCMCtrace(samples,type='trace',pdf=FALSE)
```
Each graph contains the trace plots of the last $5000$ iterations of each parameter. Therefore, the burn-in value that I will choose is $15000$.

## e.

```{r eval=FALSE}
code <- nimbleCode({
  mu0 ~ dnorm(1500,sd=500)
  sbatch ~ dunif(0,100)
  ssample ~ dunif(0,100)
  for(i in 1:nbatch){
    eps[i] ~ dnorm(0,sd=sbatch)
    mu[i] <- mu0+eps[i]
  }
  for(i in 1:N){
    y[i] ~ dnorm(mu[batch[i]],sd=ssample)
  }
})

constants <- list(nbatch=nbatch,N=N,batch=batch)
data <- list(y=y)
inits <- list(mu0=1500,sbatch=1,ssample=2,eps=rep(0,6))

Rmodel <- nimbleModel(code,constants,data,inits)
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc)

initsFunction <- function(){
  list(mu0=rnorm(1,1500,100),
       sigma_sample=runif(1,0,10),
       sigma_batch=runif(1,0,10)
      )
}

samples <- runMCMC(Cmcmc,niter=20000,nburnin=15000,nchains=3,inits=initsFunction,samplesAsCodaMCMC=TRUE)
```

```{r echo=FALSE,eval=FALSE}
save(samples,file='samples2_problem12.2.Rdata')
```

```{r echo=FALSE}
load('samples2_problem12.2.Rdata')
```

## f.

```{r}
MCMCtrace(samples,type='trace',pdf=FALSE)
```

## g.

```{r}
gelman.diag(samples)
```

## h.

```{r}
MCMCsummary(samples)
```

The 95% BCI for $mu_0$ is $(1527.02672,1576.66825)$, $(16.64776,94.19317)$ for $\sigma_{batch}$, and $(39.27440,71.70068)$ for $\sigma_{sample}$. Therefore, the true values of $\mu_0$, $\sigma_{batch}$, and $\sigma_{sample}$ will be contained in 95 out of 100 similar BCI's.

# Problem 3

## Part I

### a.

Our prior distribution is $p_i\sim Beta(1,1)$ for $i=1,2,\dots,11,12$. The likelihood is $y_i\mid p_i\sim Binomial(n_i,p_i)$ where $n$ is the number of cardiac that took place at a specific hospital.

### b.

There are 12 top-level parameters in this model: the mortality rates for each hospital.

### c.

Since we chose beta priors for each top-level parameter, then under a binomial likelihood, I would expect NIMBLE to assign conjugate samplers to each parameter.

### d.

```{r}
surgeries <- read.csv('surgeries.csv')

n <- surgeries$Surgeries
y <- surgeries$Mortalities
p <- numeric(12)
```

```{r eval=FALSE}
code <- nimbleCode({
  for(i in 1:length(n)) {
    p[i] ~ dbeta(1,1)
    y[i] ~ dbinom(size=n[i],prob=p[i])
  }
})

constants <- list(n=n)
data <- list(y=y)
inits <- list(p=runif(12))

Rmodel <- nimbleModel(code,constants,data,inits)
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc)

samples <- runMCMC(Cmcmc)
```

```{r echo=FALSE,eval=FALSE}
save(samples,file='samples_problem12.3d.Rdata')
```

```{r echo=FALSE}
load('samples_problem12.3d.Rdata')
```

Each $p_i$ was assigned as a conjugate sampler.

### e.

```{r}
MCMCtrace(samples,pdf=FALSE)
```

### f & g.

```{r}
samplesSummary(samples)
```
On average, the hospital with the highest mortality rate is the eighth hospital, with a mean mortality rate of about $14.8\%$. If we observe the original data and divide each hospital's mortalities by the number of surgeries they performed, we see that the eighth hospital had the highest mortality rate. Furthermore, the fourth hospital has the narrowest 95% BCI. The reason for this is the fourth hospital performed way more surgeries, $810$, than any other hospital.

## Part II

### h.

Our priors are $\mu\sim Normal(0,\sigma=10000)$ and $\sigma\sim Uniform(0,10000)$. Our latent state $\varepsilon_i\sim Normal(\mu,\sigma)$ for $i=1,2,\dots,11,12$. Our likelihood is calculated as $logit(p[i])=\mu+\varepsilon_i$.

### i.

There are two top-level parameters: $\mu$ and $\sigma$. They represent the mean and standard deviation of the mortality rates for each hospital.

### j.

There is one latent state: $\varepsilon_i$. It represents the potential noise in each hospital's mortality rate.

### k.

```{r eval=FALSE}
code <- nimbleCode({
  mu ~ dnorm(0,sd=10000)
  sigma ~ dunif(0,10000)
  for(i in 1:length(n)) {
    eps[i] ~ dnorm(mu,sd=sigma)
    logit(p[i]) <- mu+eps[i]
  }
})

constants <- list(n=n)
data <- list(y=y)
inits <- list(p=runif(12),mu=0,sigma=5000,eps=rep(0,12))

Rmodel <- nimbleModel(code,constants,data,inits)
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc)

samples <- runMCMC(Cmcmc)
```

```{r echo=FALSE,eval=FALSE}
save(samples,file='samples_problem12.3k.Rdata')
```

```{r echo=FALSE}
load('samples_problem12.3k.Rdata')
```

### l.

```{r}
samplesSummary(samples)
MCMCtrace(samples,params='mu',pdf=FALSE)
```

### m.

```{r}
ilogit(-71.5299)
ilogit(-20006.951)
ilogit(19904.592)
```

### n.

```{r}
sum(y)/sum(n)
```
The mean of $\mu$ in the NIMBLE model is essentially $0$, while the mean mortality rate is roughly $7.4\%$. 

## Part III

### o.

Our priors for the top-level parameters are $\beta_0\sim Gamma(0.001,0.001)$, $\beta_1\sim Gamma(0.001,0.001)$, and $\sigma\sim Uniform(0,10000)$. Our latent state's prior is $\varepsilon_i\sim Normal(\mu,\sigma)$ for $i=1,2,\dots,11,12$ where $\mu=\beta_0+\beta_1x_i$, with $x_i$ representing the Procedure column of the surgeries dataset. Our likelihood is calculated $logit(p[i])=\mu_i+\varepsilon_i$.

### p.

We have three top-level parameters: $\beta_0$, $\beta_1$, and $\sigma$. $\beta_0$ represents the intercept of our linear regression model, $\beta_1$ represents the coefficient of the procedure variable, and $\sigma$ still represents the standard deviation of the mortality rates in each hospital.

### q.

We have two latent states: $\mu_i$ and $\varepsilon_i$. $\mu_i$ represents the mean of the linear regression model and $\varepsilon_i$ represents the same thing as it did in the previous part.

### r.

```{r eval=FALSE}
x <- surgeries$Procedure

code <- nimbleCode({
  b0 ~ dnorm(0,sd=10000)
  b1 ~ dnorm(0,sd=10000)
  sigma ~ dunif(0,10000)
  for(i in 1:length(n)) {
    mu[i] <- b0+b1*x[i]
    eps[i] ~ dnorm(mu[i],sd=sigma)
    logit(p[i]) <- mu[i]+eps[i]
  }
  mu_old <- ilogit(b0)
  mu_new <- ilogit(b0+b1)
  mortality_diff <- mu_new-mu_old
})

constants <- list(n=n)
data <- list(x=x)
inits <- list(b0=0,b1=0,p=runif(12),mu=rep(0,12),sigma=5000,eps=rep(0,12))

Rmodel <- nimbleModel(code,constants,data,inits)
conf <- configureMCMC(Rmodel)
conf$addMonitors('mu_old','mu_new','mortality_diff')
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc)

samples <- runMCMC(Cmcmc,nchains=2,samplesAsCodaMCMC=TRUE)
```

```{r echo=FALSE,eval=FALSE}
save(samples,file='samples_problem12.3r.Rdata')
```

```{r echo=FALSE}
load('samples_problem12.3r.Rdata')
```

### s.

```{r}
gelman.diag(samples)
```

### t.

```{r}
MCMCsummary(samples)
```
On average, the mortality rate with the new procedure was about $5\%$ while the mortality rate for the old procedure was roughly $4.97\%$. Therefore, on average, the mortality rate using the new procedure was slightly higher than the mortality rate using the old procedure.

# Problem 4

## a.

Our priors are $\beta_0, \beta_{baseline},\beta_{treatment},\beta_{age}\sim Normal(0,\sigma=10000)$, $\sigma,\sigma_{patient}\sim Uniform(0,10000)$, $\gamma_i\sim Normal(0,\sigma_{patient})$, and $\varepsilon_{i,j}\sim Normal(0,\sigma)$. The likelihood is $y_{i,j}\sim Poisson(\lambda[i,j])$.

## b.

Our top-level parameters are the coefficients ($\beta_0$, $\beta_{baseline}$, $\beta_{treatment}$, and $\beta_{age}$) for the linear regression model as well as the standard deviations $\sigma$ and $\sigma_{patient}$. Our latent states are the $\gamma_i$'s and the $\varepsilon_{i,j}$'s for $i=1,2,\dots,58,59$ and $j=1,2,3,4$.

## c.

```{r eval=FALSE}
seizure <- read.csv('seizures.csv')

N <- 59
counts <- matrix(nrow=N,ncol=4)
counts[,1] <- seizure$s1
counts[,2] <- seizure$s2
counts[,3] <- seizure$s3
counts[,4] <- seizure$s4

baseline <- seizure$logbaseline
treatment <- seizure$treatment
age <- seizure$age

code <- nimbleCode({
  for(i in 1:4){
    b[i] ~ dnorm(0,sd=10000)
  }
  sigma ~ dunif(0,10000)
  sigma_p ~ dunif(0,10000)
  for(i in 1:N){
    gamma[i] ~ dnorm(0,sd=sigma_p)
    for(j in 1:4){
      eps[i,j] ~ dnorm(0,sd=sigma)
      log(lambda[i,j]) <- b[1]+b[2]*baseline[i]+b[3]*treatment[i]+b[4]*age[i]+gamma[i]+eps[i,j]
      y[i,j] ~ dpois(lambda[i,j])
    }
  }
})

constants <- list(N=N)
data <- list(baseline=baseline,treatment=treatment,age=age,y=counts)
inits <- list(b=rep(0,4),sigma=5000,sigma_p=5000,gamma=rep(0,59),eps=matrix(0,nrow=59,ncol=4))

Rmodel <- nimbleModel(code,constants,data,inits)
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc)

t <- system.time(samples <- runMCMC(Cmcmc,10000))[3]
```

```{r echo=FALSE,eval=FALSE}
save(samples,file='samples_problem12.4c.Rdata')
```

```{r echo=FALSE}
load('samples_problem12.4c.Rdata')
```

```{r eval=FALSE}
t/60
```
The algorithm took $0.07083333$ minutes to run.

## d.

```{r}
samplesSummary(samples)
exp(-0.2702535)
```
On average, the treatment decreases the risk of seizures per week by $0.763186$.

## e.

```{r}
MCMCtrace(samples,type='trace',pdf=FALSE)
```
All of the coefficients appear to be pretty slow mixing. The treatment coefficient did the best out of the four, but the coefficients for baseline and age, as well as the intercept, mixed very slowly.

## f.

```{r}
effectiveSize(samples)
```

## g.

```{r}
(1000/2.657395)*0.07083333
```
The run time on the slowest mixing parameter, the intercept $\beta_0$, to obtain $1000$ samples would take $26.65518$ minutes.

## h.

```{r eval=FALSE}
baseline <- seizure$logbaseline-mean(seizure$logbaseline)
age <- seizure$age-mean(seizure$age)

treatment <- ifelse(seizure$treatment==0,-0.5,0.5)

code <- nimbleCode({
  for(i in 1:4){
    b[i] ~ dnorm(0,sd=10000)
  }
  sigma ~ dunif(0,10000)
  sigma_p ~ dunif(0,10000)
  for(i in 1:N){
    gamma[i] ~ dnorm(0,sd=sigma_p)
    for(j in 1:4){
      eps[i,j] ~ dnorm(0,sd=sigma)
      log(lambda[i,j]) <- b[1]+b[2]*baseline[i]+b[3]*treatment[i]+b[4]*age[i]+gamma[i]+eps[i,j]
      y[i,j] ~ dpois(lambda[i,j])
    }
  }
})

constants <- list(N=N)
data <- list(baseline=baseline,treatment=treatment,age=age,y=counts)
inits <- list(b=rep(0,4),sigma=5000,sigma_p=5000,gamma=rep(0,59),eps=matrix(0,nrow=59,ncol=4))

Rmodel <- nimbleModel(code,constants,data,inits)
conf <- configureMCMC(Rmodel)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc)

t <- system.time(samples <- runMCMC(Cmcmc,10000))[3]
```

```{r echo=FALSE,eval=FALSE}
save(samples,file='samples_problem12.4h.Rdata')
```

```{r echo=FALSE}
load('samples_problem12.4h.Rdata')
```

```{r}
effectiveSize(samples)
```

## i.

```{r}
(1000/3.88135)*0.07266667
```

The run time on the slowest mixing parameter, now $\sigma_{patient}$, would take $18.72201$ minutes.

## j.

```{r}
samplesSummary(samples)
exp(-0.307228627)
```
On average, the treatment decreases the risk of seizures per week by $0.7354824$.

## k.

```{r eval=FALSE}
code <- nimbleCode({
  for(i in 1:4){
    b[i] ~ dnorm(0,sd=10000)
  }
  sigma ~ dunif(0,10000)
  sigma_p ~ dunif(0,10000)
  for(i in 1:N){
    gamma[i] ~ dnorm(0,sd=sigma_p)
    for(j in 1:4){
      eps[i,j] ~ dnorm(0,sd=sigma)
      log(lambda[i,j]) <- b[1]+b[2]*baseline[i]+b[3]*treatment[i]+b[4]*age[i]+gamma[i]+eps[i,j]
      y[i,j] ~ dpois(lambda[i,j])
    }
  }
})

constants <- list(N=N)
data <- list(baseline=baseline,treatment=treatment,age=age,y=counts)
inits <- list(b=rep(0,4),sigma=5000,sigma_p=5000,gamma=rep(0,59),eps=matrix(0,nrow=59,ncol=4))

Rmodel <- nimbleModel(code,constants,data,inits)
conf <- configureMCMC(Rmodel,onlySlice=TRUE)
Rmcmc <- buildMCMC(conf)

Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc)

t <- system.time(samples <- runMCMC(Cmcmc,niter=10000))[3]
effectiveSize(samples)
t <- system.time(samples <- runMCMC(Cmcmc,niter=(1000/68.51761)*25000))
```

```{r echo=FALSE,eval=FALSE}
save(samples,file='samples_problem12.4k.Rdata')
```

```{r echo=FALSE}
load('samples_problem12.4k.Rdata')
```

```{r}
effectiveSize(samples)
samplesSummary(samples)
exp(-0.195102859)
```
On average, the treatment decreased the risk of seizures per week by $0.82275$.